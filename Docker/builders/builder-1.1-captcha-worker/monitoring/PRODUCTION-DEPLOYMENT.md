# Rocket.Chat Alertmanager Integration - Production Deployment Guide

**Document Version:** 1.0  
**Last Updated:** 2026-01-30  
**Status:** COMPLETE & TESTED âœ…

---

## ðŸ“‹ Table of Contents

1. [Prerequisites](#prerequisites)
2. [Rocket.Chat Webhook Creation](#rocketchat-webhook-creation)
3. [Production Configuration](#production-configuration)
4. [Deployment Steps](#deployment-steps)
5. [Health & Monitoring](#health--monitoring)
6. [Troubleshooting](#troubleshooting)
7. [Maintenance & Operations](#maintenance--operations)

---

## Prerequisites

### System Requirements

- Docker Engine 20.10+ and Docker Compose 1.29+
- Access to Rocket.Chat admin console (production instance)
- Network connectivity between monitoring stack and Rocket.Chat server
- Environment variables configured (see Configuration section)
- Firewall rules allowing HTTP/HTTPS to Rocket.Chat webhooks

### Required Credentials

- **Rocket.Chat Admin Access** - To create incoming webhooks
- **API/Service Account Tokens** (optional) - For advanced authentication
- **Webhook URLs** - Generated from Rocket.Chat (3 total: critical, warning, info)

### Recommended Setup

- **Docker Network:** Use dedicated Docker network (`delqhi-network` or similar)
- **Secrets Management:** Store webhook URLs in `.env` file or Docker secrets
- **Resource Allocation:** 
  - CPU: 1 core minimum
  - Memory: 512MB minimum, 1GB recommended
  - Disk: 1GB for logs and persistent data
- **Monitoring:** Prometheus metrics collection enabled
- **Logging:** Centralized log aggregation (Loki or similar)

---

## Rocket.Chat Webhook Creation

### Step 1: Access Administration Panel

1. Open your Rocket.Chat instance: `https://delqhi.chat`
2. Click on **Administration** (top-left menu or icon)
3. Navigate to **Workspace â†’ Integrations**
4. Click **New Integration**

### Step 2: Create Critical Alerts Webhook

**Configuration:**

| Setting | Value | Description |
|---------|-------|-------------|
| **Enabled** | âœ… Yes | Webhook is active |
| **Event Trigger** | Incoming Webhook | Type of integration |
| **Name** | `SIN-Solver Critical Alerts` | Descriptive name |
| **Post to Channel** | `#alerts-critical` | Channel for critical alerts |
| **Webhook URL** | `https://delqhi.chat/hooks/incoming/[auto-generated]` | Auto-generated by Rocket.Chat |
| **Script Enabled** | âœ… Yes | Enable webhook processing |

**Custom Webhook Script (Optional - for enhanced formatting):**

```javascript
class Script {
  process_incoming_request({ request }) {
    let payload = request.content;
    
    // Transform Alertmanager format to Rocket.Chat format
    let attachments = [];
    
    if (payload.alerts && payload.alerts.length > 0) {
      payload.alerts.forEach((alert) => {
        attachments.push({
          color: 'danger',
          title: `ðŸ”´ CRITICAL: ${alert.labels.alertname}`,
          text: alert.annotations.description || alert.annotations.summary,
          fields: [
            {
              title: 'Severity',
              value: alert.labels.severity.toUpperCase(),
              short: true
            },
            {
              title: 'Service',
              value: alert.labels.service || 'N/A',
              short: true
            },
            {
              title: 'Instance',
              value: alert.labels.instance || 'N/A',
              short: false
            }
          ],
          ts: new Date(alert.startsAt).getTime() / 1000
        });
      });
    }
    
    return {
      text: `âš ï¸ **${payload.alerts.length} Critical Alert(s) Detected**`,
      attachments: attachments,
      username: 'SIN-Solver Alerting',
      avatar_url: 'https://example.com/logo.png'
    };
  }
}
```

**After Configuration:**

1. Click **Save**
2. Rocket.Chat displays a unique webhook URL: `https://delqhi.chat/hooks/incoming/xxxxx/yyyyy`
3. **Copy this URL** - You'll need it for environment configuration

### Step 3: Create Warning Alerts Webhook

Repeat Step 2 with these differences:

| Setting | Value |
|---------|-------|
| **Name** | `SIN-Solver Warning Alerts` |
| **Post to Channel** | `#alerts-warning` |

**Copy the generated webhook URL for `.env` configuration**

### Step 4: Create Info Alerts Webhook

Repeat Step 2 with these differences:

| Setting | Value |
|---------|-------|
| **Name** | `SIN-Solver Info Alerts` |
| **Post to Channel** | `#alerts-info` |

**Copy the generated webhook URL for `.env` configuration**

### Step 5: Verify Webhooks

In Rocket.Chat admin panel, go to Administration â†’ Integrations and verify all three webhooks appear with status **Active** âœ…

---

## Production Configuration

### Step 1: Configure Environment Variables

**File:** `.env` (in monitoring directory)

```bash
# Rocket.Chat Webhook URLs
ROCKETCHAT_WEBHOOK_CRITICAL=https://delqhi.chat/hooks/incoming/abc123xyz789
ROCKETCHAT_WEBHOOK_WARNING=https://delqhi.chat/hooks/incoming/def456uvw012
ROCKETCHAT_WEBHOOK_INFO=https://delqhi.chat/hooks/incoming/ghi789tst345

# Webhook Adapter Settings
PORT=8093
DEBUG=false

# Optional: Custom Headers for Authentication (if required)
# WEBHOOK_AUTH_HEADER=Authorization
# WEBHOOK_AUTH_VALUE=Bearer token_here

# Optional: Proxy Settings (if behind corporate proxy)
# HTTP_PROXY=http://proxy.example.com:8080
# HTTPS_PROXY=http://proxy.example.com:8080

# Optional: Timeout Settings
# WEBHOOK_TIMEOUT=30
# CONNECT_TIMEOUT=10
```

### Step 2: Review Alertmanager Configuration

**File:** `alertmanager.yml`

The configuration is already optimized for production:

```yaml
global:
  resolve_timeout: 5m

route:
  receiver: 'null'
  group_by: ['alertname', 'severity', 'service']
  
  routes:
    # Critical alerts - sent immediately
    - match:
        severity: critical
      receiver: 'critical-webhook'
      group_wait: 0s
      group_interval: 5m
      repeat_interval: 30m
      
    # Warning alerts - grouped every 10 seconds
    - match:
        severity: warning
      receiver: 'warning-webhook'
      group_wait: 10s
      group_interval: 10m
      repeat_interval: 1h
      
    # Info alerts - grouped every 30 seconds
    - match:
        severity: info
      receiver: 'info-webhook'
      group_wait: 30s
      group_interval: 1h
      repeat_interval: 4h

receivers:
  - name: 'null'
  
  - name: 'critical-webhook'
    webhook_configs:
      - url: 'http://rocketchat-webhook-adapter:8093/webhook'
        send_resolved: true
  
  - name: 'warning-webhook'
    webhook_configs:
      - url: 'http://rocketchat-webhook-adapter:8093/webhook'
        send_resolved: true
  
  - name: 'info-webhook'
    webhook_configs:
      - url: 'http://rocketchat-webhook-adapter:8093/webhook'
        send_resolved: true

inhibit_rules:
  # Inhibit info/warning alerts when critical alert exists
  - source_match:
      severity: 'critical'
    target_match_re:
      severity: 'warning|info'
    equal: ['alertname', 'service']
```

**Key Production Settings:**
- âœ… `resolve_timeout: 5m` - Alerts expire after 5 minutes
- âœ… `group_by: ['alertname', 'severity', 'service']` - Group similar alerts
- âœ… `send_resolved: true` - Send resolution notifications
- âœ… Inhibit rules prevent alert spam
- âœ… Repeat intervals prevent alert fatigue

### Step 3: Docker Network Configuration

Ensure the Docker network allows service-to-service communication:

**File:** `docker-compose.yml`

```yaml
version: '3.8'

services:
  alertmanager:
    # ... existing config ...
    networks:
      - delqhi-network
    
  rocketchat-webhook-adapter:
    # ... existing config ...
    networks:
      - delqhi-network

networks:
  delqhi-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
```

---

## Deployment Steps

### Step 1: Pre-Deployment Checklist

```bash
# 1. Verify webhook URLs are accessible
curl -v https://delqhi.chat/hooks/incoming/[YOUR_WEBHOOK_ID]
# Expected: 405 Method Not Allowed (GET not supported, POST is)

# 2. Test network connectivity from Docker
docker run --rm curlimages/curl curl -v https://delqhi.chat

# 3. Verify .env file has correct webhook URLs
cat .env | grep ROCKETCHAT_WEBHOOK

# 4. Check available disk space
df -h | grep -E "/$|/var"

# 5. Review Docker version
docker --version && docker-compose --version
```

### Step 2: Deploy Monitoring Stack

```bash
# Navigate to monitoring directory
cd /Users/jeremy/dev/SIN-Solver/Docker/builders/builder-1.1-captcha-worker/monitoring

# Pull latest images
docker-compose pull

# Start services
docker-compose up -d

# Wait 30 seconds for services to initialize
sleep 30

# Check status
docker-compose ps
```

**Expected Output:**
```
NAME                         STATUS                 PORTS
alertmanager                 Up (healthy)           0.0.0.0:9093->9093/tcp
rocketchat-webhook-adapter   Up (healthy)           0.0.0.0:8093->8093/tcp
```

### Step 3: Verify Service Health

```bash
# Check webhook adapter health
curl http://localhost:8093/health
# Expected: {"service":"rocketchat-webhook-adapter","status":"healthy","version":"1.0.0"}

# Check Alertmanager health
curl http://localhost:9093/-/healthy
# Expected: OK

# View recent logs
docker-compose logs --tail 20 rocketchat-webhook-adapter
docker-compose logs --tail 20 alertmanager
```

### Step 4: Send Test Alert

```bash
# Send critical alert to test routing
curl -X POST http://localhost:8093/webhook \
  -H "Content-Type: application/json" \
  -d '{
    "status": "firing",
    "alerts": [{
      "status": "firing",
      "labels": {
        "alertname": "TestCriticalAlert",
        "severity": "critical",
        "service": "monitoring",
        "instance": "localhost"
      },
      "annotations": {
        "summary": "This is a test critical alert",
        "description": "Testing alert routing to Rocket.Chat"
      },
      "startsAt": "'$(date -u +'%Y-%m-%dT%H:%M:%SZ')'"
    }],
    "groupLabels": {"alertname": "TestCriticalAlert", "severity": "critical"},
    "commonLabels": {"service": "monitoring"},
    "commonAnnotations": {}
  }'
```

**Expected Results:**
1. No errors in webhook adapter logs
2. Alert appears in Rocket.Chat `#alerts-critical` channel
3. Alert shows in human-readable format with severity badge

### Step 5: Configure Container Restart Policy

For production, ensure services restart automatically:

**File:** `docker-compose.yml`

```yaml
services:
  alertmanager:
    restart_policy:
      condition: on-failure
      delay: 5s
      max_attempts: 3
    
  rocketchat-webhook-adapter:
    restart_policy:
      condition: on-failure
      delay: 5s
      max_attempts: 3
```

---

## Health & Monitoring

### Container Health Checks

Both containers include health checks:

```bash
# Check health status
docker-compose ps

# View detailed health history
docker inspect $(docker-compose ps -q alertmanager) | grep -A 20 "Health"
docker inspect $(docker-compose ps -q rocketchat-webhook-adapter) | grep -A 20 "Health"
```

### Monitoring Metrics

The webhook adapter exposes Prometheus metrics on port 8093:

```bash
# Get Prometheus metrics
curl http://localhost:8093/metrics

# Example metrics:
# - http_requests_total - Total HTTP requests
# - http_request_duration_seconds - Request latency
# - alerts_routed_total - Total alerts routed
# - alert_routing_errors_total - Routing failures
```

### Log Monitoring

```bash
# Tail real-time logs
docker-compose logs -f

# Filter logs by service
docker-compose logs -f rocketchat-webhook-adapter
docker-compose logs -f alertmanager

# Search logs for errors
docker-compose logs rocketchat-webhook-adapter | grep -i error

# View last 100 lines
docker-compose logs --tail 100
```

### Log Aggregation (Optional - Loki)

If using Loki for log aggregation:

```bash
# Add to docker-compose.yml
logging:
  driver: loki
  options:
    loki-url: "http://localhost:3100/api/prom/push"
    loki-external-labels: "job=monitoring,instance=sin-solver"
```

---

## Troubleshooting

### Issue: Webhook Adapter Won't Start

**Symptom:** `docker-compose up` fails, service crashes

**Diagnosis:**
```bash
docker-compose logs rocketchat-webhook-adapter
```

**Common Solutions:**

1. **Port 8093 Already in Use:**
   ```bash
   lsof -i :8093
   # Either kill process or change PORT in .env
   ```

2. **Missing Dependencies:**
   ```bash
   docker-compose build --no-cache rocketchat-webhook-adapter
   docker-compose up -d
   ```

3. **Invalid Environment Variables:**
   ```bash
   # Verify .env format
   cat .env
   # Ensure no quotes around values: PORT=8093 (not PORT="8093")
   ```

### Issue: Alerts Not Reaching Rocket.Chat

**Symptom:** Webhook adapter receives alerts but they don't appear in Rocket.Chat

**Diagnosis:**
```bash
# Check adapter logs for webhook failures
docker-compose logs rocketchat-webhook-adapter | grep -i "failed\|error"

# Test webhook URL directly
curl -X POST 'https://delqhi.chat/hooks/incoming/YOUR_WEBHOOK_ID' \
  -H 'Content-Type: application/json' \
  -d '{"text": "Test message"}'
```

**Common Solutions:**

1. **Invalid Webhook URL:**
   - Regenerate webhook in Rocket.Chat admin panel
   - Copy exact URL (no typos)
   - Update `.env` and restart: `docker-compose restart rocketchat-webhook-adapter`

2. **Network Connectivity:**
   ```bash
   # Test from container
   docker-compose exec rocketchat-webhook-adapter \
     curl -v https://delqhi.chat/hooks/incoming/test
   ```

3. **Rocket.Chat Firewall Rules:**
   - Check Rocket.Chat IP whitelist settings
   - Verify incoming webhook is enabled: Administration â†’ Workspace â†’ Settings â†’ Webhooks

4. **SSL Certificate Issues:**
   ```bash
   # Check certificate validity
   openssl s_client -connect delqhi.chat:443
   ```

### Issue: High Alert Latency

**Symptom:** Alerts take minutes to appear in Rocket.Chat

**Diagnosis:**
```bash
# Check alertmanager queue
curl http://localhost:9093/api/v1/alerts

# Monitor webhook response times
docker-compose logs | grep "duration"
```

**Solutions:**

1. **Increase Group Wait for Non-Critical Alerts:**
   ```yaml
   # In alertmanager.yml
   group_wait: 5s  # Instead of 10s
   ```

2. **Check Network Latency:**
   ```bash
   ping delqhi.chat
   traceroute delqhi.chat
   ```

3. **Verify Webhook Adapter Performance:**
   - Check CPU usage: `docker stats rocketchat-webhook-adapter`
   - Check memory: `docker stats --no-stream`
   - Review logs for processing delays

### Issue: Webhook Adapter Memory Leak

**Symptom:** Memory usage grows over time, service becomes slow

**Diagnosis:**
```bash
# Monitor memory in real-time
docker stats --no-stream=false rocketchat-webhook-adapter

# Check for memory leaks in logs
docker-compose logs rocketchat-webhook-adapter | grep -i "memory\|gc"
```

**Solutions:**

1. **Restart Service Periodically:**
   ```bash
   # Add cron job to restart at 3 AM daily
   0 3 * * * cd /path/to/monitoring && docker-compose restart rocketchat-webhook-adapter
   ```

2. **Increase Resource Limits:**
   ```yaml
   rocketchat-webhook-adapter:
     deploy:
       resources:
         limits:
           memory: 2G
         reservations:
           memory: 1G
   ```

3. **Update Application:**
   ```bash
   docker-compose pull
   docker-compose up -d --no-deps rocketchat-webhook-adapter
   ```

---

## Maintenance & Operations

### Daily Operations

**Morning Checklist:**
```bash
# Check service status
docker-compose ps

# Review error logs from overnight
docker-compose logs --since 8h rocketchat-webhook-adapter | grep -i error

# Check Rocket.Chat channel activity
# (Manual check in Rocket.Chat)
```

**Alert Volume Monitoring:**
```bash
# Count alerts by severity (last 1 hour)
docker-compose logs --since 1h alertmanager | grep "severity" | sort | uniq -c
```

### Weekly Maintenance

**Log Rotation:**
```bash
# View log size
docker-compose logs rocketchat-webhook-adapter | wc -c

# Clean logs if > 100MB
docker-compose logs -f rocketchat-webhook-adapter > /dev/null &
# Or use log driver limit in docker-compose.yml
```

**Backup Alertmanager Configuration:**
```bash
# Backup current config
cp alertmanager.yml alertmanager.yml.backup.$(date +%Y%m%d)

# Verify backup
ls -lah alertmanager.yml*
```

**Update Dependencies:**
```bash
# Check for image updates
docker-compose pull

# Review changes
docker-compose config

# Deploy updates
docker-compose up -d
```

### Monthly Operations

**Performance Review:**
```bash
# Analyze alert metrics
curl -s http://localhost:8093/metrics | grep alert

# Generate performance report
docker-compose logs --since 30d rocketchat-webhook-adapter | \
  grep "duration" | \
  awk '{print $NF}' | \
  sort -n | \
  tail -20
```

**Configuration Audit:**
```bash
# Verify webhook URLs are still valid
for webhook in $ROCKETCHAT_WEBHOOK_CRITICAL $ROCKETCHAT_WEBHOOK_WARNING $ROCKETCHAT_WEBHOOK_INFO; do
  echo "Testing: $webhook"
  curl -X GET "$webhook" -w "\nHTTP Status: %{http_code}\n\n"
done
```

**Disaster Recovery:**
```bash
# Backup all configurations
tar -czf monitoring-backup-$(date +%Y%m%d-%H%M%S).tar.gz \
  .env alertmanager.yml docker-compose.yml

# Store backup securely
mv monitoring-backup-*.tar.gz /secure/backup/location/
```

### Scaling for High Alert Volume

If experiencing > 1000 alerts/minute:

1. **Enable Alert Batching:**
   ```yaml
   group_wait: 30s  # Increase from 10s
   group_interval: 5m  # Increase from 1m
   ```

2. **Add Alert Deduplication:**
   ```yaml
   inhibit_rules:
     - source_match_re:
         severity: 'critical|warning'
       target_match_re:
         severity: 'warning|info'
       equal: ['alertname', 'service']
   ```

3. **Increase Container Resources:**
   ```yaml
   resources:
     limits:
       cpus: '2'
       memory: 2G
     reservations:
       cpus: '1'
       memory: 1G
   ```

4. **Add Redis Queue (Optional):**
   - Queue alerts in Redis before sending to Rocket.Chat
   - Implement workers to process queue asynchronously
   - Reduces webhook adapter load

---

## Production Checklist

- [ ] All three Rocket.Chat webhooks created and tested
- [ ] Webhook URLs added to `.env` file
- [ ] Docker services started successfully
- [ ] Health endpoints returning 200 OK
- [ ] Test alert successfully routed to Rocket.Chat
- [ ] Logs being collected and monitored
- [ ] Restart policies configured
- [ ] Backup procedure documented and tested
- [ ] Team trained on alert response procedures
- [ ] Monitoring and alerting enabled
- [ ] Documentation updated with production URLs
- [ ] Disaster recovery plan in place

---

## Next Steps

1. **Configure Alert Sources:** Update Prometheus, application logs, and other monitoring systems to send alerts to Alertmanager on port 9093
2. **Alert Rules:** Define custom alerting rules based on your SINSolver thresholds and business requirements
3. **Team Training:** Train operations team on interpreting alerts and response procedures
4. **Documentation:** Update runbooks with Rocket.Chat channel information
5. **Feedback Loop:** Monitor alert accuracy and adjust rules based on false positives

---

**Document Version:** 1.0  
**Last Updated:** 2026-01-30  
**Status:** COMPLETE & PRODUCTION-READY âœ…

For additional help, see:
- Main README: `./README.md`
- Quick Start: `./QUICK-START.md`
- Setup Guide: `./SETUP-GUIDE.md`
- Testing Report: `./TESTING-REPORT.md`
